# -*- coding: utf-8 -*-
"""Batch1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-iPWJ_9v2mrk3nUzYWfp4U5Zpdqj4O7F
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import random
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Read CSV file into a Pandas DataFrame
df = pd.read_csv("cyber crime(2017-2020).csv")

# Display the DataFrame
df

# Display summary statistics
df.describe()

# Print the number of rows and columns
num_rows, num_columns = df.shape
print("Number of rows:", num_rows)
print("Number of columns:", num_columns)

# Data Cleaning: Drop specified rows and set new index
df.drop([29, 37, 38, 68, 76, 77, 78, 108, 116, 117, 146, 155, 156], axis=0, inplace=True)
a = [i for i in range(144)]
df['Index'] = a
df.set_index('Index', inplace=True)

# Display the DataFrame after cleaning
num_rows, num_columns = df.shape
print("Number of rows:", num_rows)
print("Number of columns:", num_columns)

# Visualization: Heatmap of correlation matrix
plt.figure(figsize=(50, 25))
sns.heatmap(df.corr(), annot=True)
plt.show()

# Visualization: Boxplots and scatter plots for numeric columns
num = df.select_dtypes(include=[np.number]).columns
plt.figure(figsize=(100, 100))
sns.boxplot(df[num])

plt.figure(figsize=(20, 8))
sns.scatterplot(df[num])

# Statistical Analysis: Calculate mean, median, quartiles, min, max, and mode for numeric columns
num = df.select_dtypes(include=[np.number])
mean = num.mean()
median = num.median()
quartiles = num.quantile([0.25, 0.5, 0.75])
min_val = num.min()
max_val = num.max()
mode = df.mode()

# Print statistical results
print("\nMean:\n\n", mean)
print("\nMedian:\n\n", median)
print("\nQuartiles:\n\n", quartiles)
print("\nMin:\n\n", min_val)
print("\nMax:\n\n", max_val)
print("\nMode:\n\n", mode)

# Identify and print outliers for each numeric column
numeric = df.select_dtypes(include=np.number)
for i in numeric.columns:
    column = numeric[i]
    Q1 = column.quantile(0.25)
    Q3 = column.quantile(0.75)
    IQR = Q3 - Q1
    lower_threshold = Q1 - 1.5 * IQR
    upper_threshold = Q3 + 1.5 * IQR
    outliers = column[(column < lower_threshold) | (column > upper_threshold)]
    print("\n\nOutliers of", i, ":\n")
    print(outliers)

# Check and print missing values
print(df.isna().sum())

# Handling Missing Values: Fill missing values with mean for numeric columns and mode for categorical columns
numeric_columns = df.select_dtypes(include=np.number).columns
for i in numeric_columns:
    df[i] = df[i].fillna(df[i].mean())
categorical_columns = df.select_dtypes(include='object').columns
for i in categorical_columns:
    df[i] = df[i].fillna(df[i].mode()[0])

# Check missing values after filling
print(df.isna().sum())

# Dictionary Creation: Create a dictionary of states and their corresponding cybercrime cases in the year 2020
dictionary = {}
for i in range(len(df)):
    if df.iloc[i, 25] == 2020:
        if df.iloc[i, 1] == 'State':
            dictionary[df.iloc[i, 2]] = df.iloc[i, 24]

# Find and print the state with the highest cases in 2020
d = sorted(dictionary.values(), reverse=True)
for i, j in dictionary.items():
    if j == d[0]:
        s = i
print('State that has the highest cases in the year 2020:', s)

# Pie Chart: Plot a pie chart for total cybercrime cases in 2019 and 2020
mdc = {2019: 0, 2020: 0}
for i in range(len(df)):
    if df.iloc[i, 1] != 'State':
        if df.iloc[i, 25] == 2019:
            mdc[2019] += df.iloc[i, 24]
        elif df.iloc[i, 25] == 2020:
            mdc[2020] += df.iloc[i, 24]
k = [mdc[2019], mdc[2020]]
plt.pie(k, labels=[f'2019 cases : {k[0]}', f'2020 cases : {k[1]}'])

# One-Hot Encoding and Label Encoding
encoded = pd.get_dummies(df, columns=['Category', 'State/UT'])
l = LabelEncoder()
X = encoded.drop(columns=['Year'], axis=1)
y = encoded['Year']
L = l.fit_transform(y)

# Feature Selection: Use Chi-squared test for feature selection and print scores
s = SelectKBest(score_func=chi2, k=3)
s.fit_transform(X, L)
score = pd.Series(s.scores_, index=X.columns)
print(score)

# Print top 3 features based on scores
top3_features = score.nlargest(3)
print("Top 3 features:")
print(top3_features)

# Binning: Bin the 'Year' column into 20 bins and replace values with bin means
df['bins'] = pd.qcut(df['Year'], 20, duplicates='drop')
print('\nBinning\n', df['Year'])
bin_means = df.groupby('bins')['Year'].mean()
df['Year'] = df['bins'].map(bin_means)
df.drop('bins', axis=1, inplace=True)
print('\n\nAfter Binning\n', df['Year'])

# Train-Test Split and Decision Tree Classification
X1, X2, y1, y2 = train_test_split(X, y, train_size=0.4, random_state=42)
d = DecisionTreeClassifier(max_depth=10000)
d.fit(X1, y1)
ypre = d.predict(X2)
print(accuracy_score(ypre, y2))
