# -*- coding: utf-8 -*-
"""CSA_Lab_Prep_CA_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tA6XVXWNWL6p9un6hyh14kUDYT8NJiLL
"""

import pandas as pd
import numpy as np

df =pd.read_excel("Internet_Crime_Final_Version.xlsx")

num_rows, num_columns = df.shape

print("Number of rows:", num_rows)
print("Number of columns:", num_columns)

print(list(df['State']))

print("\nS.No      ColumnName                    Datatype")
for k,(i,j) in enumerate(df.dtypes.items(),start=1):
  print(f"{k:<10}{i:<30}{j}")

print(df.describe())

print(list(df['Age<59_count']))

for i in range(len(df)):
  df.iloc[i,24] = int(str(df.iloc[i,24]).strip().replace(',',''))
print(list(df['Age<59_count']))
df['Age<59_count'] = df['Age<59_count'].astype(int)

print("\nS.No      ColumnName                    Datatype")
for k,(i,j) in enumerate(df.dtypes.items(),start=1):
  print(f"{k:<10}{i:<30}{j}")

encoded = pd.get_dummies(df, columns=['State'])

#df.drop('s/n ',axis=1, inplace=True)
#encoded.drop('s/n ',axis=1, inplace=True)

num_rows, num_columns = encoded.shape

print("Number of rows:", num_rows)
print("Number of columns:", num_columns)

print("\nS.No      ColumnName                    Datatype")
for k,(i,j) in enumerate(encoded.dtypes.items(),start=1):
  print(f"{k:<10}{i:<30}{j}")

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(50,25))
sns.heatmap(df.corr(), annot=True)
plt.title('For Data With Objects')
plt.show()

plt.figure(figsize=(100,50))
sns.heatmap(encoded.corr(), annot=True)
plt.title('For Encoded Data Without Objects')
plt.show()

summary = df.describe()

min_val = summary.loc['min']
first_quartile = summary.loc['25%']
median = summary.loc['50%']
third_quartile = summary.loc['75%']
max_val = summary.loc['max']

print("\nMinimum:", min_val)
print("\nFirst Quartile (Q1):", first_quartile)
print("\nMedian (Q2):", median)
print("\nThird Quartile (Q3):", third_quartile)
print("\nMaximum:", max_val)

summary = encoded.describe()

min_val = summary.loc['min']
first_quartile = summary.loc['25%']
median = summary.loc['50%']
third_quartile = summary.loc['75%']
max_val = summary.loc['max']

print("\nMinimum:", min_val)
print("\nFirst Quartile (Q1):", first_quartile)
print("\nMedian (Q2):", median)
print("\nThird Quartile (Q3):", third_quartile)
print("\nMaximum:", max_val)

for i in encoded.columns:
  print("\n\nBoxPlot of ",i)
  plt.boxplot(encoded[i])
  plt.show()

for i in encoded.columns:
  column = encoded[i]

  Q1 = column.quantile(0.25)
  Q3 = column.quantile(0.75)

  IQR = Q3 - Q1

  lower_threshold = Q1 - 1.5 * IQR
  upper_threshold = Q3 + 1.5 * IQR

  outliers = column[(column < lower_threshold) | (column > upper_threshold)]

  print("\n\nOutliers:\n")
  print(outliers)

# For numerical columns, replace missing values with the mean
numeric_columns = df.select_dtypes(include=np.number).columns
for i in numeric_columns:
    df[i] = df[i].fillna(df[i].mean())

# For categorical columns, replace missing values with the mode
for i in df.select_dtypes(include='object').columns:
    df[i] = df[i].fillna(df[i].mode()[0])

from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.preprocessing import MinMaxScaler

d = df.drop('State',axis=1)
scaler = MinMaxScaler()
e = scaler.fit_transform(d)
print('\nAfter Min-Max Normalization\n',e)

df['2016_bins']=pd.qcut(df['Age>60_count'], 20)
print('\nBining\n',df['Age>60_count'])
bin_means=df.groupby('2016_bins')['Age>60_count'].mean()
df['Age>60_count']=df['2016_bins'].map(bin_means)
df.drop('2016_bins',axis=1,inplace=True)
print('\n\nAfter Binning\n',df['Age>60_count'])

import random

n = []
for i in range(len(encoded)):
  x = random.randint(0,1)
  if x==0:
    n.append('Low')
  else:
    n.append('High')

encoded['Class'] = n

l = LabelEncoder()
X = encoded.drop(columns=['Class'], axis=1)
y = encoded['Class']

L = l.fit_transform(y)

s = SelectKBest(score_func=chi2, k=3)
s.fit_transform(X, L)
score = pd.Series(s.scores_, index=X.columns)
print(score)

top3_features = score.nlargest(3)
print("Top 3 features:")
print(top3_features)

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score

X1,X2,y1,y2 = train_test_split(X,y,train_size=0.3,random_state=42)
d = DecisionTreeClassifier(max_depth=4)
d.fit(X1,y1)
y_pred = d.predict(X2)
print(accuracy_score(y2, y_pred))

loss_sum = df.filter(like='_loss').sum()

highest_impact_loss = loss_sum.idxmax()
lowest_impact_loss = loss_sum.idxmin()

highest_impact_sum = loss_sum.max()
lowest_impact_sum = loss_sum.min()

print("Loss with highest impact overall:", highest_impact_loss)
print("Sum of highest impact loss:", highest_impact_sum)
print("Loss with lowest impact overall:", lowest_impact_loss)
print("Sum of lowest impact loss:", lowest_impact_sum)

highest_impact_losses = loss_sum.nlargest(2)
lowest_impact_losses = loss_sum.nsmallest(2)

print("Top two losses with the highest impact overall:")
print(highest_impact_losses)

print("\nTop two losses with the lowest impact overall:")
print(lowest_impact_losses)

selected_column = 'Population'

column_mean = df[selected_column].mean()
filtered_records = df[df[selected_column] < column_mean]

print("Records where '{}' column value is less than the mean ({}):".format(selected_column, column_mean))
print(filtered_records)